- ## Smoothing, Interpolation, and Backoff
	- **C√≥ m·ªôt v·∫•n ƒë·ªÅ khi s·ª≠ d·ª•ng ∆∞·ªõc l∆∞·ª£ng kh·∫£ nƒÉng c·ª±c ƒë·∫°i cho x√°c su·∫•t:** b·∫•t k·ª≥ t·∫≠p h·ª£p hu·∫•n luy·ªán h·ªØu h·∫°n n√†o c≈©ng s·∫Ω thi·∫øu m·ªôt s·ªë chu·ªói t·ª´ ti·∫øng Anh
	- Khi d√πng **Maximum Likelihood Estimation (MLE)**, n·∫øu m·ªôt chu·ªói t·ª´ **kh√¥ng xu·∫•t hi·ªán trong t·∫≠p hu·∫•n luy·ªán**, th√¨ x√°c su·∫•t c·ªßa n√≥ s·∫Ω b·∫±ng **0**
	- Nh∆∞ng ƒë√¢y l√† v·∫•n ƒë·ªÅ l·ªõn: Corpus lu√¥n **h·ªØu h·∫°n**, Trong khi ng√¥n ng·ªØ th√¨ **v√¥ h·∫°n v√† s√°ng t·∫°o**.
	- **H·∫≠u qu·∫£ nghi√™m tr·ªçng**: khi t√≠nh x√°c su·∫•t c·∫£ c√¢u b·∫±ng c√°ch nh√¢n c√°c x√°c su·∫•t: ch·ªâ c·∫ßn **m·ªôt n-gram c√≥ x√°c su·∫•t 0** ‚Üí C·∫£ c√¢u c√≥ x√°c su·∫•t 0.
	- **ƒêi·ªÅu n√†y khi·∫øn:**
		- Perplexity b·ªã v√¥ h·∫°n (ngh·ªãch ƒë·∫£o 0)
		- M√¥ h√¨nh ƒë√°nh gi√° sai nh·ªØng c√¢u h·ª£p l·ªá
		- Kh√¥ng th·ªÉ t·ªïng qu√°t h√≥a t·ªët
- ### ^^Laplace smoothing^^
	- **simplest way**
	- ^^add-one^^: m·ªói ƒë·∫øm n-gram ƒë·∫∑t th√†nh 1
		- l√†m cho MLE l·ªõn h∆°n 0
	- Laplace smoothing kh√¥ng ƒë·ªß t·ªët d·ªÉ d√πng trong c√°c m√¥ h√¨nh n-gram hi·ªán ƒë·∫°i
	- Nh∆∞ng n√≥ h·ªØu √≠ch trong vi·ªác gi·ªõi thi·ªáu nhi·ªÅu kh√°i ni·ªám m√† ch√∫ng ta th·∫•y trong c√°c thu·∫≠t to√°n l√†m m·ªãn kh√°c, cung c·∫•p m·ªôt m·ªëc tham chi·∫øu h·ªØu √≠ch, v√† c≈©ng l√† m·ªôt thu·∫≠t to√°n l√†m m·ªãn th·ª±c ti·ªÖn cho c√°c nhi·ªám v·ª• kh√°c nh∆∞ ph√¢n lo·∫°i vƒÉn b·∫£n (Ph·ª• l·ª•c K).  !? *deo lien quan*
	- **unsmoothed maximum likelihood estimate:**
	-
$$P(w_i) = \frac{c_i}{N}$$	- **Laplace smoothing:**
	-
$$P_{Laplace}(w_i) = \frac{c_i + 1}{N + V}$$	- v·ªõi $V$ l√† s·ªë t·ª´ trong vocabulary
	- **MLE tr∆∞·ªõc laplace smoothing:**
	-
$$P_{MLE}(w_n | w_{n-1}) = \frac{C(w_{n-1}w_n)}{C(w_{n-1})}$$	- **MLE sau laplace smoothing:**
	-
$$P_{Laplace}(w_n | w_{n-1}) = \frac{C(w_{n-1}w_n) + 1}{C(w_{n-1}) + V} = \frac{C^*(w_{n-1}w_n)}{C(w_{n-1})}$$	- Thay v√¨ nh√¨n tr·ª±c ti·∫øp v√†o x√°c su·∫•t ƒë√£ smooth, ta c√≥ th·ªÉ quy ƒë·ªïi n√≥ v·ªÅ m·ªôt ‚Äúcount m·ªõi‚Äù $C^*$ .
	- T·ª´ bi·ªÉu th·ª©c tr√™n, suy ra:
	-
$$C^*(w_{n-1}w_n) = \frac{(C(w_{n-1}w_n) + 1) \cdot C(w_{n-1})}{C(w_{n-1}) + V}$$	- L∆∞u √Ω r·∫±ng l√†m m∆∞·ª£t c·ªông m·ªôt ƒë√£ t·∫°o ra s·ª± thay ƒë·ªïi r·∫•t l·ªõn ƒë·ªëi v·ªõi c√°c t·∫ßn s·ªë. So s√°nh H√¨nh 3.8 v·ªõi c√°c t·∫ßn s·ªë g·ªëc trong H√¨nh 3.1
		- ·ªõi c√°c t·∫ßn s·ªë g·ªëc trong H√¨nh 3.1
	- Khi nh√¨n v√†o **discount** d, ƒë∆∞·ª£c ƒë·ªãnh nghƒ©a l√† t·ª∑ l·ªá gi·ªØa c√°c t·∫ßn s·ªë m·ªõi v√† c≈©, cho ch√∫ng ta th·∫•y m·ª©c ƒë·ªô gi·∫£m ƒë√°ng k·ªÉ c·ªßa c√°c t·∫ßn s·ªë cho m·ªói t·ª´ ti·ªÅn t·ªë (chi·∫øt kh·∫•u cho bigram 'mu·ªën m·ªôt' l√† 0,39, trong khi chi·∫øt kh·∫•u cho 'm√≥n ƒÉn Trung Qu·ªëc' l√† 0,10, g·∫•p 10 l·∫ßn)
	- S·ª± thay ƒë·ªïi m·∫°nh m·∫Ω n√†y x·∫£y ra v√¨ qu√° nhi·ªÅu x√°c su·∫•t ƒë√£ ƒë∆∞·ª£c chuy·ªÉn sang t·∫•t c·∫£ c√°c gi√° tr·ªã b·∫±ng kh√¥ng.
	- ‚ÄúL√Ω do l√† v√¨ add-one smoothing c·ªông th√™m 1 cho t·∫•t c·∫£ c√°c bigram, k·ªÉ c·∫£ nh·ªØng bigram ch∆∞a t·ª´ng xu·∫•t hi·ªán.
	- Do vocabulary r·∫•t l·ªõn, s·ªë l∆∞·ª£ng bigram c√≥ count b·∫±ng 0 l√† c·ª±c k·ª≥ nhi·ªÅu.
	- Khi c·ªông 1 cho t·∫•t c·∫£ ch√∫ng, ta ph·∫£i l·∫•y m·ªôt l∆∞·ª£ng x√°c su·∫•t r·∫•t l·ªõn t·ª´ c√°c bigram ph·ªï bi·∫øn ƒë·ªÉ ph√¢n cho nh·ªØng bigram ch∆∞a th·∫•y.
	- V√¨ v·∫≠y c√°c bigram quan tr·ªçng b·ªã gi·∫£m m·∫°nh.‚Äù
- ### ^^Add-k smoothing^^
	- ^^add-k^^: t∆∞∆°ng t·ª± nh∆∞ laplace smoothing nh∆∞ng thay v√¨ th√™m 1 th√¨ th√™m k k√≠ t·ª±
	-
$$P^*_{Add-k}(w_n | w_{n-1}) = \frac{C(w_{n-1}w_n) + k}{C(w_{n-1}) + kV}$$	- c·∫ßn 1 ph∆∞∆°ng ph√°p ƒë·ªÉ ch·ªçn k: vd ch·ªçn tr√™n **devset**
		- **Devset (development set)** l√† t·∫≠p d·ªØ li·ªáu d√πng ƒë·ªÉ:
		- üëâ ƒêi·ªÅu ch·ªânh m√¥ h√¨nh
		- üëâ Ch·ªçn hyperparameter
		- üëâ So s√°nh c√°c phi√™n b·∫£n model
	- H·ªØu √≠ch cho 1 v√†i c√¥ng vi·ªác nh∆∞ng kh√¥ng l√†m t·ªët v·ªõi language modeling, t·∫°o s·ªë li·ªáu v·ªõi ph∆∞∆°ng sai k√©m v√† th∆∞·ªùng discount kh√¥ng ph√π h·ª£p
- ### ^^Language Model Interpolation^^
	- Trong tr∆∞·ªùng h·ª£p n·∫øu ng·ªØ c·∫£nh d√†i qu√° m√† kh√¥ng c√≥ d·ªØ li·ªáu, 1 c√°ch hay l√† ta **gi·∫£m b·ªõt ng·ªØ c·∫£nh** ƒë·ªÉ c√≥ nhi·ªÅu d·ªØ li·ªáu h∆°n
	- N·∫øu kh√¥ng c√≥ **trigram**:
	-
$$P(w_n \mid w_{n-2} w_{n-1})$$	- Ta d√πng **bigram**:
	-
$$P(w_n \mid w_{n-1})$$	- N·∫øu **bigram** c≈©ng kh√¥ng c√≥, ta d√πng **unigram**:
	-
$$P(w_n)$$	- ^^interpolation^^:
		- T√≠nh x√°c su·∫•t m·ªõi b·∫±ng c√°ch n·ªôi suy c√°c x√°c su·∫•t tri-gram, bi-gram v√† unigram.
		-
$$P(w_n \mid w_{n-2} w_{n-1}) = \lambda_1 P(w_n) + \lambda_2 P(w_n \mid w_{n-1}) + \lambda_3 P(w_n \mid w_{n-2} w_{n-1})$$		- v·ªõi $$\sum \lambda_i = 1$$
	- **slightly more sophisticated version**:
		- M·ªói tr·ªçng s·ªë Œª ƒë∆∞·ª£c t√≠nh d·ª±a v√†o ng·ªØ c·∫£nh:
			- N·∫øu m·ªôt bigram c√≥ r·∫•t nhi·ªÅu d·ªØ li·ªáu,
				- ‚Üí trigram d·ª±a tr√™n n√≥ ƒë√°ng tin h∆°n
				- ‚Üí ta tƒÉng tr·ªçng s·ªë cho trigram.
			- N·∫øu trigram hi·∫øm,
				- ‚Üí ta gi·∫£m tr·ªçng s·ªë c·ªßa n√≥
				- ‚Üí d·ª±a nhi·ªÅu h∆°n v√†o bigram ho·∫∑c unigram.
			-
$$P(w_n | w_{n-2}w_{n-1}) = \lambda_1(w_{n-2:n-1})P(w_n) + \lambda_2(w_{n-2:n-1})P(w_n | w_{n-1}) + \lambda_3(w_{n-2:n-1})P(w_n | w_{n-2}w_{n-1})$$	- ch·ªçn Œª:
		- ^^held-out^^ corpus: l√† t·∫≠p h·ª£p d·ªØ li·ªáu hu·∫•n luy·ªán b·ªï sung, s·ª≠ d·ª•ng ƒë·ªÉ x√°c l·∫≠p c√°c gi√° tr·ªã Œª
		- => Ch√∫ng ta c·ªë ƒë·ªãnh c√°c x√°c su·∫•t n-gram v√† sau ƒë√≥ t√¨m ki·∫øm c√°c gi√° tr·ªã Œª m√†‚Äîkhi ƒë∆∞·ª£c ƒë∆∞a v√†o ph∆∞∆°ng tr√¨nh 3.29‚Äîs·∫Ω cho ch√∫ng ta x√°c su·∫•t cao nh·∫•t c·ªßa t·∫≠p d·ªØ li·ªáu gi·ªØ l·∫°i
		- C√≥ nhi·ªÅu c√°ch kh√°c nhau ƒë·ªÉ t√¨m b·ªô gi√° tr·ªã Œª t·ªëi ∆∞u n√†y. M·ªôt c√°ch l√† s·ª≠ d·ª•ng thu·∫≠t to√°n EM, m·ªôt thu·∫≠t to√°n h·ªçc l·∫∑p ƒëi l·∫∑p l·∫°i h·ªôi t·ª• t·ªõi c√°c gi√° tr·ªã Œª t·ªëi ∆∞u c·ª•c b·ªô (Jelinek v√† Mercer, 1980).
- ### Stupid Backoff
	- ^^backoff^^: n·∫øu n-gram c√≥ zero counts th√¨ l√πi v·ªÅ (n-1) gram, ti·∫øp t·ª•c nh∆∞ v·∫≠y cho t·ªõi h·∫øt zero counts.
	- ^^discount^^: gi·∫£m tr·ªçng s·ªë c√°c n-gram b·∫≠c cao ƒë·ªÉ gi·ªØ l·∫°i m·ªôt ph·∫ßn x√°c su·∫•t cho c√°c n-gram b·∫≠c th·∫•p h∆°n.
		- T·∫°i sao ph·∫£i c·∫ßn discount -> answerr b√™n d∆∞·ªõi
		- V·∫•n ƒë·ªÅ: N·∫øu gi·ªØ nguy√™n x√°c su·∫•t trigram theo MLE ‚Üí t·ªïng x√°c su·∫•t c·ªßa t·∫•t c·∫£ c√°c t·ª´ b·∫±ng 1 -> khi g·∫∑p trigram ch∆∞a th·∫•y v√† mu·ªën backoff xu·ªëng bigram th√¨ kh√¥ng c√≤n ‚Äúph·∫ßn x√°c su·∫•t d∆∞‚Äù ƒë·ªÉ c·∫•p cho bigram n·ªØa.
	- ^^stupid backoff^^:
		- T·ª´ b·ªè √Ω t∆∞·ªüng c·ªë g·∫Øng bi·∫øn m√¥ h√¨nh ng√¥n ng·ªØ th√†nh m·ªôt ph√¢n ph·ªëi x√°c su·∫•t th·ª±c s·ª±.
		- N·∫øu m·ªôt n-gram b·∫≠c cao c√≥ s·ªë l∆∞·ª£ng ƒë·∫øm b·∫±ng kh√¥ng, ch√∫ng ta ƒë∆°n gi·∫£n quay v·ªÅ n-gram b·∫≠c th·∫•p h∆°n, ƒë∆∞·ª£c c√¢n b·∫±ng b·∫±ng m·ªôt tr·ªçng s·ªë c·ªë ƒë·ªãnh (kh√¥ng ph·ª• thu·ªôc v√†o ng·ªØ c·∫£nh).
		-
$$P(w_i | w_{i-N+1:i-1}) =
		  \begin{cases}
		  \frac{\text{count}(w_{i-N+1:i})}{\text{count}(w_{i-N+1:i-1})} & \text{if } \text{count}(w_{i-N+1:i}) > 0 \
		  \lambda P(w_i | w_{i-N+2:i-1}) & \text{otherwise}
		  \end{cases}$$		- Ph∆∞∆°ng ph√°p backoff k·∫øt th√∫c ·ªü unigram, c√≥ ƒëi·ªÉm $$S(w) = count(w) / N$$
-
- ## *nh·∫Øc em H∆∞ng:*
	- n-gram l√† g√¨
	- estimate probabilities by Maximum Likelihood Estimation (MLE)
	- unigram, biagram
	- perplexity
	-
